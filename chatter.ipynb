{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d693aa2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuyeoreum/miniconda3/envs/tts/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 85307.88it/s]\n",
      "/home/yuyeoreum/miniconda3/envs/tts/lib/python3.11/site-packages/diffusers/models/lora.py:393: FutureWarning: `LoRACompatibleLinear` is deprecated and will be removed in version 1.0.0. Use of `LoRACompatibleLinear` is deprecated. Please switch to PEFT backend by installing PEFT: `pip install peft`.\n",
      "  deprecate(\"LoRACompatibleLinear\", \"1.0.0\", deprecation_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded PerthNet (Implicit) at step 250,000\n"
     ]
    }
   ],
   "source": [
    "import torchaudio as ta\n",
    "from chatterbox.mtl_tts import ChatterboxMultilingualTTS\n",
    "import torch\n",
    "from g2pk import G2p\n",
    "g2p = G2p()\n",
    "\n",
    "multilingual_model = ChatterboxMultilingualTTS.from_pretrained(device=\"cuda\")\n",
    "# seed = 123\n",
    "# torch.manual_seed(seed)\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21872d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분명 우리느 놀바른 때에 모두 최서늘 다해, 가장 올바른 선태글 래렫짜나\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:  14%|█▎        | 136/1000 [00:02<00:17, 48.19it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  14%|█▎        | 136/1000 [00:02<00:18, 46.89it/s]\n"
     ]
    }
   ],
   "source": [
    "text = \"분명 우리는 올바른 때에 모두 최선을 다해, 가장 올바른 선택을 내렸잖아\"\n",
    "phonemes = g2p(text)\n",
    "print(phonemes)\n",
    "\n",
    "wav_korean = multilingual_model.generate(phonemes, language_id=\"ko\", audio_prompt_path=\"audio_ref/miyako2.wav\")#, cfg_weight=0.3, exaggeration=0.4, temperature=0.5)\n",
    "ta.save(\"버전2.wav\", wav_korean.cpu(), multilingual_model.sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7dca73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "# Chatterbox 출력 파일 로드\n",
    "# librosa는 오디오 데이터를 numpy 배열 형태로 다룹니다.\n",
    "y, sr = librosa.load(\"test-korean.wav\")\n",
    "\n",
    "# 핵심 기능: top_db 값을 기준으로 앞/뒤 무음 구간을 잘라냄\n",
    "# top_db: 오디오의 최대 볼륨을 기준으로, 이 값보다 작은 데시벨의 구간을 무음으로 간주.\n",
    "# 숫자가 클수록 더 작은 소리까지 '유음'으로 판단하여 덜 잘라냅니다. (보통 20~40 사이에서 조절)\n",
    "y_trimmed, index = librosa.effects.trim(y, top_db=30)\n",
    "\n",
    "# 잘라낸 오디오를 새로운 파일로 저장\n",
    "sf.write(\"test-kokrean_trimmed.wav\", y_trimmed, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1f0813b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:  20%|██        | 202/1000 [00:03<00:14, 53.24it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  21%|██        | 206/1000 [00:04<00:15, 51.35it/s]\n"
     ]
    }
   ],
   "source": [
    "french_text = \"Bonjour, comment ça va? Ceci est le modèle de synthèse vocale multilingue Chatterbox, il prend en charge 23 langues.\"\n",
    "wav_french = multilingual_model.generate(french_text, language_id=\"fr\", audio_prompt_path=\"miyako.wav\")\n",
    "ta.save(\"test-french.wav\", wav_french, multilingual_model.sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c31bfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:  10%|▉         | 95/1000 [00:01<00:17, 50.90it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  10%|▉         | 98/1000 [00:01<00:17, 52.17it/s]\n"
     ]
    }
   ],
   "source": [
    "japanese_text = \"どうか なくさないでよって 高架下 過ぎる日々を\"\n",
    "wav_japanese = multilingual_model.generate(japanese_text, language_id=\"ja\", audio_prompt_path=\"miyako.wav\")\n",
    "ta.save(\"test-japanese.wav\", wav_japanese, multilingual_model.sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78797b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuyeoreum/miniconda3/envs/tts/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/home/yuyeoreum/miniconda3/envs/tts/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   8%|▊         | 83/1000 [00:01<00:19, 47.40it/s]\n"
     ]
    }
   ],
   "source": [
    "japanese_text = \"どうか なくさないでよって 高架下 過ぎる日々を\"\n",
    "wav_japanese = multilingual_model.generate(japanese_text, language_id=\"ja\", audio_prompt_path=\"moon.wav\", exaggeration=0.3, cfg_weight=0.3)\n",
    "ta.save(\"test-japanese.wav\", wav_japanese, multilingual_model.sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7944ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_text = \"Bonjour, comment ça va? Ceci est le modèle de synthèse vocale multilingue Chatterbox, il prend en charge 23 langues.\"\n",
    "wav_french = multilingual_model.generate(french_text, language_id=\"fr\")\n",
    "ta.save(\"test-french.wav\", wav_french, model.sr)\n",
    "\n",
    "chinese_text = \"你好，今天天气真不错，希望你有一个愉快的周末。\"\n",
    "wav_chinese = multilingual_model.generate(chinese_text, language_id=\"zh\")\n",
    "ta.save(\"test-chinese.wav\", wav_chinese, model.sr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
